{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing,Cleansing and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have Two seperate data file as of now\n",
    " 1. overall consolidated data(rows = 16408095,columns = 59),going forward we will refer it as **consol_df**.\n",
    " 2. credit card data(rows = 5470942, columns = 7), we will refer it as **credit_df**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First will start with consol_df into which will drop few columns which has high NA's and which is not important for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 09:17:27 WARN Utils: Your hostname, amolmurme resolves to a loopback address: 127.0.1.1; using 192.168.1.9 instead (on interface wlp3s0)\n",
      "22/11/09 09:17:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/09 09:17:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 476 ms, sys: 974 ms, total: 1.45 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading the Overall consolidated file\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Bilad Retail EDA\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"14g\").getOrCreate()\n",
    "consol_df = spark.read.parquet(\"consolidated_08_nov/consolidated_08.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rows = 16408095 and columns = 59\n"
     ]
    }
   ],
   "source": [
    "# Check the count of Rows and columns\n",
    "print(f' Rows = {consol_df.count()} and columns = {len(consol_df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contract_no', 'sector_2', 'residence', 'customer_id', 'amount', 'installment', 'value_date', 'fin_mat_date', 'category', 'drawdown_net_amt', 'total_profit', 'rate', 'period', 'month_profit', 'salary_amount', 'occupation', 'company_name', 'city', 'sector_19', 'employment_date', 'pd_amount', 'outstanding_finance', 'days', 'x1st_pd_creation_date', 'product_description', 'asset_price', 'balloon_payment', 'birth_date', 'gender', 'martial', 'education_level', 'id', 'a_c_open_date', 'x1st_installment_date', 'service_period_yearly', 'sector_21', 'pd_days', 'down_payment', 'salary_assignment', 'occupation_type', 'remaining_inst', 'remaining_profit_amt', 'simah_score', 'behaviuor_score', 'application_score', 'no_of_dependents', 'x43', 'seller_branch', 'sector_22', 'old_maturity_date', 'last_postpone_execution_date', 'last_postpone_by_no_of_months', 'total_no_of_times_postpone_executed', 'old_maturity_date_before_postpone', 'last_installment_amt_postponed', 'nationality', 'id_date', 'dpd_created', 'def_flag'] 59\n"
     ]
    }
   ],
   "source": [
    "print(consol_df.columns,len(consol_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns having High NA's and not so necessary for our analysis based on the EDA done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contract_no', 'sector_2', 'residence', 'customer_id', 'amount', 'installment', 'value_date', 'fin_mat_date', 'category', 'drawdown_net_amt', 'total_profit', 'rate', 'period', 'month_profit', 'salary_amount', 'occupation', 'company_name', 'city', 'employment_date', 'pd_amount', 'outstanding_finance', 'product_description', 'asset_price', 'birth_date', 'gender', 'martial', 'education_level', 'id', 'x1st_installment_date', 'service_period_yearly', 'pd_days', 'down_payment', 'salary_assignment', 'occupation_type', 'remaining_inst', 'remaining_profit_amt', 'simah_score', 'behaviuor_score', 'application_score', 'no_of_dependents', 'old_maturity_date', 'last_installment_amt_postponed', 'nationality', 'id_date', 'dpd_created', 'def_flag'] 46\n"
     ]
    }
   ],
   "source": [
    "drop_cols=[\"sector_22\",\"sector_21\", \"a_c_open_date\", \"days\", \"sector_19\", \"x43\", \"seller_branch\", \"x1st_pd_creation_date\", \"balloon_payment\", \"old_maturity_date_before_postpone\", \"last_postpone_execution_date\", \"last_postpone_by_no_of_months\", \"total_no_of_times_postpone_executed\",\"fwd_def_tag\"]\n",
    "consol_df=consol_df.drop(*drop_cols)\n",
    "print(consol_df.columns,len(consol_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+\n",
      "|dpd_created|def_flag| pd_days|\n",
      "+-----------+--------+--------+\n",
      "|   14541961|       0|15898097|\n",
      "+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df_Columns=[\"dpd_created\",\"def_flag\",\"pd_days\"]\n",
    "consol_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contract_no: string (nullable = true)\n",
      " |-- id_date: timestamp (nullable = true)\n",
      " |-- sector_2: string (nullable = true)\n",
      " |-- residence: string (nullable = true)\n",
      " |-- customer_id: double (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- value_date: string (nullable = true)\n",
      " |-- fin_mat_date: string (nullable = true)\n",
      " |-- category: integer (nullable = true)\n",
      " |-- drawdown_net_amt: double (nullable = true)\n",
      " |-- total_profit: double (nullable = true)\n",
      " |-- rate: double (nullable = true)\n",
      " |-- period: integer (nullable = true)\n",
      " |-- month_profit: double (nullable = true)\n",
      " |-- salary_amount: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- employment_date: string (nullable = true)\n",
      " |-- pd_amount: double (nullable = true)\n",
      " |-- outstanding_finance: double (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      " |-- asset_price: double (nullable = true)\n",
      " |-- birth_date: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- martial: string (nullable = true)\n",
      " |-- education_level: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- x1st_installment_date: timestamp (nullable = true)\n",
      " |-- service_period_yearly: integer (nullable = true)\n",
      " |-- pd_days: integer (nullable = true)\n",
      " |-- down_payment: double (nullable = true)\n",
      " |-- salary_assignment: string (nullable = true)\n",
      " |-- occupation_type: string (nullable = true)\n",
      " |-- remaining_inst: integer (nullable = true)\n",
      " |-- remaining_profit_amt: double (nullable = true)\n",
      " |-- simah_score: double (nullable = true)\n",
      " |-- behaviuor_score: integer (nullable = true)\n",
      " |-- application_score: integer (nullable = true)\n",
      " |-- no_of_dependents: integer (nullable = true)\n",
      " |-- old_maturity_date: integer (nullable = true)\n",
      " |-- last_installment_amt_postponed: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- dpd_created: integer (nullable = true)\n",
      " |-- def_flag: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consol_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consol_df.select('category').distinct().count() # Find out total distinct category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segmentation will be done on the mapping given by ganesh which uses 'category' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "personal finance = category(21059,21060,**21063**,21064,21065,21066,21072)>>2103>>product description>>\n",
    "1. LSF Against Real Estate Mortgage\n",
    "2. LSFAgainstRealEstateMortgageTOPUP\"\n",
    "\n",
    "Auto Lease\t= category(21067)\n",
    "\n",
    "Real Estate = category(21061,21062,**21063**,21069,21073)2103>>product description>>\n",
    "1. Self Construction Finance\n",
    "2. Topup Construction Finance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when,udf\n",
    "from pyspark.sql. types import StringType,IntegerType\n",
    "personal_finance = [21059,21060,21064,21065,21066,21072]\n",
    "Auto_Lease\t= [21067]\n",
    "Real_Estate = [21061,21062,21069,21073]\n",
    "\n",
    "# def portfolio_segmentation(category):\n",
    "#     if category in personal_finance:   \n",
    "#         return \"personal finance\"\n",
    "#     elif category in Auto_Lease:\n",
    "#         return \"auto lease\"\n",
    "#     elif category in Real_Estate:\n",
    "#         return \"real estate\"\n",
    "\n",
    "\n",
    "# fun = udf(portfolio_segmentation,StringType())\n",
    "# consol_df = consol_df.withColumn(\"portfolio\",fun(consol_df.category))\n",
    "\n",
    "# condition_PF = (col(\"category\").isin(personal_finance))\n",
    "# condition_AL = (col(\"category\").isin(Auto_Lease))\n",
    "# condition_RE = (col(\"category\").isin(Real_Estate))\n",
    "\n",
    "consol_df = consol_df.withColumn('portfolio',\n",
    " when(col('category').isin(personal_finance),'personal finance')\n",
    ".when(col('category').isin(Auto_Lease),'auto lease')\n",
    ".when(col('category').isin(Real_Estate),'real estate')\n",
    ".when((col('category') == 21063) & ((col('product_description') == 'LSF Against Real Estate Mortgage')|(col('product_description') == 'LSFAgainstRealEstateMortgageTOPUP')),'personal finance')\n",
    ".when((col('category') == 21063) & ((col('product_description') == 'Self Construction Finance')|(col('product_description') == 'Topup Construction Finance')),'real estate')\n",
    "\n",
    ")\n",
    "#consol_df = consol_df.withColumn('portfolio',condition)\n",
    "#consol_df = consol_df.withColumn(\"portfolio\", when(condition_PF, \"personal finance\")).withColumn(\"portfolio\", when(condition_AL, \"auto lease\")).withColumn(\"portfolio\", when(condition_RE, \"real estate\"))\n",
    "\n",
    "\n",
    "\n",
    "#consol_df = consol_df.withColumn(\"portfolio\", (when(condition_PF, \"personal finance\") |when(condition_AL, \"auto lease\")|when(condition_RE, \"real estate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=======================>                                 (5 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|       portfolio|   count|\n",
      "+----------------+--------+\n",
      "|     real estate| 1766111|\n",
      "|            null|  356728|\n",
      "|personal finance|13231831|\n",
      "|      auto lease| 1053425|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consol_df.groupBy('portfolio').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|category|  count|\n",
      "+--------+-------+\n",
      "|   21060|  36251|\n",
      "|   21061| 834765|\n",
      "|   21063|1413397|\n",
      "|   21067|1053425|\n",
      "|   21065|1802152|\n",
      "|   21072|3503804|\n",
      "|   21068|     30|\n",
      "|   21062|   6456|\n",
      "|   21066|   8189|\n",
      "|   21064|7614446|\n",
      "|   21059|    123|\n",
      "|   21073|  52597|\n",
      "|   21069|  81602|\n",
      "|   21058|    858|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consol_df.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------+\n",
      "|product_description                |count  |\n",
      "+-----------------------------------+-------+\n",
      "|Real Estate Ijara                  |6449   |\n",
      "|Auto Lease                         |1053429|\n",
      "|Motor Vehicle Finance              |168    |\n",
      "|Buy Out Finance                    |368523 |\n",
      "|Auto Lease Portfolio               |30     |\n",
      "|Staff loan without profit          |35620  |\n",
      "|Re-Finance Re-Shced                |49021  |\n",
      "|Car Financing                      |8017   |\n",
      "|Advance Salary                     |276    |\n",
      "|Re- Finance by LSF                 |1611817|\n",
      "|Real Estate Financing              |317111 |\n",
      "|LSF Against Real Estate Mortgage   |233172 |\n",
      "|Personal Loan Top Up Finance       |3110665|\n",
      "|Local Share Murabaha Finance       |6694473|\n",
      "|Real Estate TopUp Financing        |511330 |\n",
      "|LSF ReDesign                       |491    |\n",
      "|Off Plan (Fix Installment)         |28020  |\n",
      "|LSFAgainstRealEstateMortgageTOPUP  |33711  |\n",
      "|Self Construction Finance          |193755 |\n",
      "|LSF With Mortgage TopUp Installment|2      |\n",
      "+-----------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consol_df.groupBy('product_description').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets read the credit card data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add 'portfolio' column and also need to current_default_tagging based on Days. this will be contract_no level default tagging.\n",
    "\n",
    "Once this is done we can append the credit card data with consolidated data and then will do customer level default tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "credit_df = spark.read.csv('CC_base_data_28Oct2022.csv',sep='\\t',inferSchema=True,header=True,escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CIF: integer (nullable = true)\n",
      " |-- TOTAL_OUTSTANDING_BALANCE: double (nullable = true)\n",
      " |-- Days: integer (nullable = true)\n",
      " |-- CARD_PRODUCT: string (nullable = true)\n",
      " |-- Limit: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Contract.ID: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470942"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find count for empty, None, Null, Nan with string literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Contract.ID' as contract_no\n",
    "credit_df = credit_df.withColumnRenamed('Contract.ID','contract_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+------------+-----+-----------+\n",
      "|CIF|TOTAL_OUTSTANDING_BALANCE| Days|CARD_PRODUCT|Limit|contract_no|\n",
      "+---+-------------------------+-----+------------+-----+-----------+\n",
      "|  0|                        0|69213|     1847969| 2480|    1162090|\n",
      "+---+-------------------------+-----+------------+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "columns = ['CIF',\n",
    " 'TOTAL_OUTSTANDING_BALANCE',\n",
    " 'Days',\n",
    " 'CARD_PRODUCT',\n",
    " 'Limit',\n",
    " 'contract_no']\n",
    "df2 = credit_df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT Date)|\n",
      "+--------------------+\n",
      "|                 105|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "credit_df.select(countDistinct('Date')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ccf4353c41d9f02b300c0f8079db5824c63936791799eda44b7b3c04299904c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
